{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import dotdict\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "args.model = 'FEDformer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "args.task_id = '2test'\n",
    "args.is_training = 1\n",
    "args.version = 'Fourier'\n",
    "args.task = 'test'\n",
    "args.data = 'custom' # data\n",
    "args.root_path = './dataset/' # root path of data file\n",
    "args.data_path = 'Informer-data.csv' # data file\n",
    "args.features = 'M' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.target = 'Austria' # target feature in S or MS task\n",
    "args.continent = 'Europe'\n",
    "args.freq = 'm' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.detail_freq = args.freq\n",
    "args.freq = args.freq[-1:]\n",
    "args.checkpoints = './informer_checkpoints' # location of model checkpoints\n",
    "\n",
    "args.mode_select = 'random'\n",
    "args.modes = 32\n",
    "args.seq_len = 32 # input sequence length of Informer encoder\n",
    "args.label_len = 16# start token length of Informer decoder\n",
    "args.pred_len = 1 # prediction sequence length\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "\n",
    "args.enc_in = 97 # encoder input size\n",
    "args.dec_in = 97 # decoder input size\n",
    "args.c_out  = 97 # output size\n",
    "args.factor = 2 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 8 # num of heads\n",
    "args.e_layers = 2 # num of encoder layers\n",
    "args.d_layers = 1 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.do_predict = True\n",
    "args.moving_avg = 12\n",
    "args.cross_activation = 'tanh'\n",
    "args.base = 'legendre'\n",
    "\n",
    "args.batch_size = 4\n",
    "args.learning_rate = 0.0001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False # whether to use automatic mixed precision training\n",
    "\n",
    "args.num_workers = 0\n",
    "args.itr = 1\n",
    "args.train_epochs = 250\n",
    "args.patience = 5\n",
    "args.des = 'exp'\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() else False\n",
    "args.gpu = 0\n",
    "\n",
    "args.use_multi_gpu = False\n",
    "args.devices = '0,1,2,3'\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(args.is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hej\n",
      "Use GPU: cuda:0\n",
      "fourier enhanced block used!\n",
      "modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "fourier enhanced block used!\n",
      "modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      " fourier enhanced cross attention used!\n",
      "modes_q=8, index_q=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "modes_kv=16, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "enc_modes: 16, dec_modes: 8\n",
      ">>>>>>>start training : 014_2test_FEDformer_8_batchsizerandom_modes32_custom_ftM_sl32_ll16_pl1_dm512_nh8_el2_dl1_df2048_fc2_ebtimeF_dtTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 184\n",
      "val 24\n",
      "test 0\n",
      "Epoch: 1 cost time: 7.883534669876099\n",
      "Epoch: 1, Steps: 23 | Train Loss: 1.2875114 Vali Loss: 0.9432063 Test Loss: nan\n",
      "Validation loss decreased (inf --> 0.943206).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "Epoch: 2 cost time: 2.5935378074645996\n",
      "Epoch: 2, Steps: 23 | Train Loss: 0.9561442 Vali Loss: 0.8917972 Test Loss: nan\n",
      "Validation loss decreased (0.943206 --> 0.891797).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "Epoch: 3 cost time: 2.2672765254974365\n",
      "Epoch: 3, Steps: 23 | Train Loss: 0.8024045 Vali Loss: 0.8777003 Test Loss: nan\n",
      "Validation loss decreased (0.891797 --> 0.877700).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "Epoch: 4 cost time: 2.571772336959839\n",
      "Epoch: 4, Steps: 23 | Train Loss: 0.7419548 Vali Loss: 0.8691537 Test Loss: nan\n",
      "Validation loss decreased (0.877700 --> 0.869154).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "Epoch: 5 cost time: 2.205031394958496\n",
      "Epoch: 5, Steps: 23 | Train Loss: 0.7182723 Vali Loss: 0.8679197 Test Loss: nan\n",
      "Validation loss decreased (0.869154 --> 0.867920).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "Epoch: 6 cost time: 2.566291570663452\n",
      "Epoch: 6, Steps: 23 | Train Loss: 0.7025920 Vali Loss: 0.8670502 Test Loss: nan\n",
      "Validation loss decreased (0.867920 --> 0.867050).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "Epoch: 7 cost time: 2.5306625366210938\n",
      "Epoch: 7, Steps: 23 | Train Loss: 0.7003840 Vali Loss: 0.8664106 Test Loss: nan\n",
      "Validation loss decreased (0.867050 --> 0.866411).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "Epoch: 8 cost time: 2.7268223762512207\n",
      "Epoch: 8, Steps: 23 | Train Loss: 0.6920537 Vali Loss: 0.8661380 Test Loss: nan\n",
      "Validation loss decreased (0.866411 --> 0.866138).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "Epoch: 9 cost time: 3.1401376724243164\n",
      "Epoch: 9, Steps: 23 | Train Loss: 0.6904744 Vali Loss: 0.8660074 Test Loss: nan\n",
      "Validation loss decreased (0.866138 --> 0.866007).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "Epoch: 10 cost time: 2.9671497344970703\n",
      "Epoch: 10, Steps: 23 | Train Loss: 0.6911296 Vali Loss: 0.8659363 Test Loss: nan\n",
      "Validation loss decreased (0.866007 --> 0.865936).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      "Epoch: 11 cost time: 2.4182801246643066\n",
      "Epoch: 11, Steps: 23 | Train Loss: 0.6916589 Vali Loss: 0.8659074 Test Loss: nan\n",
      "Validation loss decreased (0.865936 --> 0.865907).  Saving model ...\n",
      "Updating learning rate to 9.765625e-08\n",
      "Epoch: 12 cost time: 1.9964101314544678\n",
      "Epoch: 12, Steps: 23 | Train Loss: 0.6898697 Vali Loss: 0.8658957 Test Loss: nan\n",
      "Validation loss decreased (0.865907 --> 0.865896).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-08\n",
      "Epoch: 13 cost time: 2.1173551082611084\n",
      "Epoch: 13, Steps: 23 | Train Loss: 0.6891804 Vali Loss: 0.8658872 Test Loss: nan\n",
      "Validation loss decreased (0.865896 --> 0.865887).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-08\n",
      "Epoch: 14 cost time: 2.116344928741455\n",
      "Epoch: 14, Steps: 23 | Train Loss: 0.6907698 Vali Loss: 0.8658816 Test Loss: nan\n",
      "Validation loss decreased (0.865887 --> 0.865882).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-08\n",
      "Epoch: 15 cost time: 2.0492660999298096\n",
      "Epoch: 15, Steps: 23 | Train Loss: 0.6927061 Vali Loss: 0.8658800 Test Loss: nan\n",
      "Validation loss decreased (0.865882 --> 0.865880).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-09\n",
      "Epoch: 16 cost time: 2.5453951358795166\n",
      "Epoch: 16, Steps: 23 | Train Loss: 0.6913499 Vali Loss: 0.8658791 Test Loss: nan\n",
      "Validation loss decreased (0.865880 --> 0.865879).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-09\n",
      "Epoch: 17 cost time: 2.961068868637085\n",
      "Epoch: 17, Steps: 23 | Train Loss: 0.6921813 Vali Loss: 0.8658788 Test Loss: nan\n",
      "Validation loss decreased (0.865879 --> 0.865879).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-09\n",
      "Epoch: 18 cost time: 2.153042793273926\n",
      "Epoch: 18, Steps: 23 | Train Loss: 0.6866534 Vali Loss: 0.8658786 Test Loss: nan\n",
      "Validation loss decreased (0.865879 --> 0.865879).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-10\n",
      "Epoch: 19 cost time: 2.039482831954956\n",
      "Epoch: 19, Steps: 23 | Train Loss: 0.6891975 Vali Loss: 0.8658785 Test Loss: nan\n",
      "Validation loss decreased (0.865879 --> 0.865879).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-10\n",
      "Epoch: 20 cost time: 2.8583579063415527\n",
      "Epoch: 20, Steps: 23 | Train Loss: 0.6893369 Vali Loss: 0.8658785 Test Loss: nan\n",
      "Validation loss decreased (0.865879 --> 0.865879).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-10\n",
      "Epoch: 21 cost time: 2.5989880561828613\n",
      "Epoch: 21, Steps: 23 | Train Loss: 0.6882949 Vali Loss: 0.8658784 Test Loss: nan\n",
      "Validation loss decreased (0.865879 --> 0.865878).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-11\n",
      "Epoch: 22 cost time: 2.064197540283203\n",
      "Epoch: 22, Steps: 23 | Train Loss: 0.6892081 Vali Loss: 0.8658784 Test Loss: nan\n",
      "Validation loss decreased (0.865878 --> 0.865878).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-11\n",
      "Epoch: 23 cost time: 2.305755853652954\n",
      "Epoch: 23, Steps: 23 | Train Loss: 0.6891382 Vali Loss: 0.8658785 Test Loss: nan\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 2.384185791015625e-11\n",
      "Epoch: 24 cost time: 2.1624956130981445\n",
      "Epoch: 24, Steps: 23 | Train Loss: 0.6874276 Vali Loss: 0.8658785 Test Loss: nan\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.1920928955078126e-11\n",
      "Epoch: 25 cost time: 2.0207056999206543\n",
      "Epoch: 25, Steps: 23 | Train Loss: 0.6901770 Vali Loss: 0.8658785 Test Loss: nan\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 5.960464477539063e-12\n",
      "Epoch: 26 cost time: 1.9242277145385742\n",
      "Epoch: 26, Steps: 23 | Train Loss: 0.6913320 Vali Loss: 0.8658785 Test Loss: nan\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 2.9802322387695314e-12\n",
      "Epoch: 27 cost time: 3.0483734607696533\n",
      "Epoch: 27, Steps: 23 | Train Loss: 0.6919420 Vali Loss: 0.8658785 Test Loss: nan\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>predicting : 014_2test_FEDformer_8_batchsizerandom_modes32_custom_ftM_sl32_ll16_pl1_dm512_nh8_el2_dl1_df2048_fc2_ebtimeF_dtTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "pred 1\n",
      "Use GPU: cuda:0\n",
      "fourier enhanced block used!\n",
      "modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "fourier enhanced block used!\n",
      "modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      " fourier enhanced cross attention used!\n",
      "modes_q=8, index_q=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "modes_kv=16, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "enc_modes: 16, dec_modes: 8\n",
      ">>>>>>>start training : 015_2test_FEDformer_8_batchsizerandom_modes32_custom_ftM_sl32_ll16_pl1_dm512_nh8_el2_dl1_df2048_fc2_ebtimeF_dtTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 184\n",
      "val 24\n",
      "test 0\n",
      "Epoch: 1 cost time: 2.4899895191192627\n",
      "Epoch: 1, Steps: 23 | Train Loss: 1.2997754 Vali Loss: 0.9643676 Test Loss: nan\n",
      "Validation loss decreased (inf --> 0.964368).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "Epoch: 2 cost time: 2.1658036708831787\n",
      "Epoch: 2, Steps: 23 | Train Loss: 0.9640016 Vali Loss: 0.9039308 Test Loss: nan\n",
      "Validation loss decreased (0.964368 --> 0.903931).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "Epoch: 3 cost time: 2.191179037094116\n",
      "Epoch: 3, Steps: 23 | Train Loss: 0.8104631 Vali Loss: 0.8864208 Test Loss: nan\n",
      "Validation loss decreased (0.903931 --> 0.886421).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "Epoch: 4 cost time: 1.9827492237091064\n",
      "Epoch: 4, Steps: 23 | Train Loss: 0.7487811 Vali Loss: 0.8803196 Test Loss: nan\n",
      "Validation loss decreased (0.886421 --> 0.880320).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "Epoch: 5 cost time: 2.085712194442749\n",
      "Epoch: 5, Steps: 23 | Train Loss: 0.7191640 Vali Loss: 0.8775603 Test Loss: nan\n",
      "Validation loss decreased (0.880320 --> 0.877560).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "Epoch: 6 cost time: 2.4895617961883545\n",
      "Epoch: 6, Steps: 23 | Train Loss: 0.7087249 Vali Loss: 0.8761570 Test Loss: nan\n",
      "Validation loss decreased (0.877560 --> 0.876157).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "Epoch: 7 cost time: 2.391176462173462\n",
      "Epoch: 7, Steps: 23 | Train Loss: 0.7004761 Vali Loss: 0.8756605 Test Loss: nan\n",
      "Validation loss decreased (0.876157 --> 0.875660).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "Epoch: 8 cost time: 2.3666956424713135\n",
      "Epoch: 8, Steps: 23 | Train Loss: 0.6989971 Vali Loss: 0.8753305 Test Loss: nan\n",
      "Validation loss decreased (0.875660 --> 0.875331).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "Epoch: 9 cost time: 2.310734272003174\n",
      "Epoch: 9, Steps: 23 | Train Loss: 0.7000536 Vali Loss: 0.8751891 Test Loss: nan\n",
      "Validation loss decreased (0.875331 --> 0.875189).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "Epoch: 10 cost time: 2.443260669708252\n",
      "Epoch: 10, Steps: 23 | Train Loss: 0.6986637 Vali Loss: 0.8751006 Test Loss: nan\n",
      "Validation loss decreased (0.875189 --> 0.875101).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      "Epoch: 11 cost time: 2.179508686065674\n",
      "Epoch: 11, Steps: 23 | Train Loss: 0.6982262 Vali Loss: 0.8750787 Test Loss: nan\n",
      "Validation loss decreased (0.875101 --> 0.875079).  Saving model ...\n",
      "Updating learning rate to 9.765625e-08\n",
      "Epoch: 12 cost time: 1.9361307621002197\n",
      "Epoch: 12, Steps: 23 | Train Loss: 0.6980679 Vali Loss: 0.8750570 Test Loss: nan\n",
      "Validation loss decreased (0.875079 --> 0.875057).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-08\n",
      "Epoch: 13 cost time: 2.2393765449523926\n",
      "Epoch: 13, Steps: 23 | Train Loss: 0.6976964 Vali Loss: 0.8750486 Test Loss: nan\n",
      "Validation loss decreased (0.875057 --> 0.875049).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-08\n",
      "Epoch: 14 cost time: 2.3624143600463867\n",
      "Epoch: 14, Steps: 23 | Train Loss: 0.6937034 Vali Loss: 0.8750434 Test Loss: nan\n",
      "Validation loss decreased (0.875049 --> 0.875043).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-08\n",
      "Epoch: 15 cost time: 2.3991785049438477\n",
      "Epoch: 15, Steps: 23 | Train Loss: 0.6970721 Vali Loss: 0.8750412 Test Loss: nan\n",
      "Validation loss decreased (0.875043 --> 0.875041).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-09\n",
      "Epoch: 16 cost time: 2.172877550125122\n",
      "Epoch: 16, Steps: 23 | Train Loss: 0.6966248 Vali Loss: 0.8750401 Test Loss: nan\n",
      "Validation loss decreased (0.875041 --> 0.875040).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-09\n",
      "Epoch: 17 cost time: 2.0665364265441895\n",
      "Epoch: 17, Steps: 23 | Train Loss: 0.6965489 Vali Loss: 0.8750396 Test Loss: nan\n",
      "Validation loss decreased (0.875040 --> 0.875040).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-09\n",
      "Epoch: 18 cost time: 2.3334591388702393\n",
      "Epoch: 18, Steps: 23 | Train Loss: 0.6988858 Vali Loss: 0.8750394 Test Loss: nan\n",
      "Validation loss decreased (0.875040 --> 0.875039).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-10\n",
      "Epoch: 19 cost time: 2.2183120250701904\n",
      "Epoch: 19, Steps: 23 | Train Loss: 0.6970702 Vali Loss: 0.8750394 Test Loss: nan\n",
      "Validation loss decreased (0.875039 --> 0.875039).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-10\n",
      "Epoch: 20 cost time: 2.8154494762420654\n",
      "Epoch: 20, Steps: 23 | Train Loss: 0.6957553 Vali Loss: 0.8750393 Test Loss: nan\n",
      "Validation loss decreased (0.875039 --> 0.875039).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-10\n",
      "Epoch: 21 cost time: 2.416076421737671\n",
      "Epoch: 21, Steps: 23 | Train Loss: 0.7003040 Vali Loss: 0.8750394 Test Loss: nan\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9.5367431640625e-11\n",
      "Epoch: 22 cost time: 2.4999873638153076\n",
      "Epoch: 22, Steps: 23 | Train Loss: 0.6941303 Vali Loss: 0.8750394 Test Loss: nan\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 4.76837158203125e-11\n",
      "Epoch: 23 cost time: 2.23136043548584\n",
      "Epoch: 23, Steps: 23 | Train Loss: 0.6933690 Vali Loss: 0.8750394 Test Loss: nan\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 2.384185791015625e-11\n",
      "Epoch: 24 cost time: 2.3191685676574707\n",
      "Epoch: 24, Steps: 23 | Train Loss: 0.6984142 Vali Loss: 0.8750393 Test Loss: nan\n",
      "Validation loss decreased (0.875039 --> 0.875039).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-11\n",
      "Epoch: 25 cost time: 2.4484763145446777\n",
      "Epoch: 25, Steps: 23 | Train Loss: 0.6987616 Vali Loss: 0.8750394 Test Loss: nan\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5.960464477539063e-12\n",
      "Epoch: 26 cost time: 2.079526901245117\n",
      "Epoch: 26, Steps: 23 | Train Loss: 0.6975005 Vali Loss: 0.8750394 Test Loss: nan\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 2.9802322387695314e-12\n",
      "Epoch: 27 cost time: 2.1161530017852783\n",
      "Epoch: 27, Steps: 23 | Train Loss: 0.6961707 Vali Loss: 0.8750394 Test Loss: nan\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.4901161193847657e-12\n",
      "Epoch: 28 cost time: 2.4607620239257812\n",
      "Epoch: 28, Steps: 23 | Train Loss: 0.6959950 Vali Loss: 0.8750394 Test Loss: nan\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 7.450580596923828e-13\n",
      "Epoch: 29 cost time: 3.3588814735412598\n",
      "Epoch: 29, Steps: 23 | Train Loss: 0.6931175 Vali Loss: 0.8750394 Test Loss: nan\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>predicting : 015_2test_FEDformer_8_batchsizerandom_modes32_custom_ftM_sl32_ll16_pl1_dm512_nh8_el2_dl1_df2048_fc2_ebtimeF_dtTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "pred 1\n",
      "Use GPU: cuda:0\n",
      "fourier enhanced block used!\n",
      "modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "fourier enhanced block used!\n",
      "modes=32, index=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      " fourier enhanced cross attention used!\n",
      "modes_q=8, index_q=[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "modes_kv=16, index_kv=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "enc_modes: 16, dec_modes: 8\n",
      ">>>>>>>start training : 016_2test_FEDformer_8_batchsizerandom_modes32_custom_ftM_sl32_ll16_pl1_dm512_nh8_el2_dl1_df2048_fc2_ebtimeF_dtTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 184\n",
      "val 24\n",
      "test 0\n",
      "Epoch: 1 cost time: 3.145859718322754\n",
      "Epoch: 1, Steps: 23 | Train Loss: 1.3061366 Vali Loss: 0.9190199 Test Loss: nan\n",
      "Validation loss decreased (inf --> 0.919020).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "Epoch: 2 cost time: 2.871528387069702\n",
      "Epoch: 2, Steps: 23 | Train Loss: 0.9677662 Vali Loss: 0.8547564 Test Loss: nan\n",
      "Validation loss decreased (0.919020 --> 0.854756).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "Epoch: 3 cost time: 3.6303884983062744\n",
      "Epoch: 3, Steps: 23 | Train Loss: 0.8115861 Vali Loss: 0.8374264 Test Loss: nan\n",
      "Validation loss decreased (0.854756 --> 0.837426).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "Epoch: 4 cost time: 3.2944533824920654\n",
      "Epoch: 4, Steps: 23 | Train Loss: 0.7522351 Vali Loss: 0.8322752 Test Loss: nan\n",
      "Validation loss decreased (0.837426 --> 0.832275).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m exp \u001b[38;5;241m=\u001b[39m Exp(args)  \u001b[38;5;66;03m# set experiments\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>start training : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting))\n\u001b[1;32m---> 42\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdo_predict:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>predicting : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting))\n",
      "File \u001b[1;32mc:\\Users\\osahl\\Desktop\\UNI\\master thesis\\FEDFormer\\exp\\exp_main.py:141\u001b[0m, in \u001b[0;36mExp_Main.train\u001b[1;34m(self, setting)\u001b[0m\n\u001b[0;32m    139\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch_x, batch_x_mark, dec_inp, batch_y_mark)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_mark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_inp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y_mark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m f_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMS\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    144\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch_y[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpred_len:, f_dim:]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\osahl\\miniconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\osahl\\miniconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\osahl\\Desktop\\UNI\\master thesis\\FEDFormer\\models\\FEDformer.py:129\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask, dec_self_mask, dec_enc_mask)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# enc\u001b[39;00m\n\u001b[0;32m    128\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_embedding(x_enc, x_mark_enc)\n\u001b[1;32m--> 129\u001b[0m enc_out, attns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_self_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# dec\u001b[39;00m\n\u001b[0;32m    131\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_embedding(seasonal_init, x_mark_dec)\n",
      "File \u001b[1;32mc:\\Users\\osahl\\miniconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\osahl\\miniconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\osahl\\Desktop\\UNI\\master thesis\\FEDFormer\\layers\\Autoformer_EncDec.py:140\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, attn_mask)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attn_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_layers:\n\u001b[1;32m--> 140\u001b[0m         x, attn \u001b[38;5;241m=\u001b[39m \u001b[43mattn_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m         attns\u001b[38;5;241m.\u001b[39mappend(attn)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\osahl\\miniconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\osahl\\miniconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\osahl\\Desktop\\UNI\\master thesis\\FEDFormer\\layers\\Autoformer_EncDec.py:106\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[1;34m(self, x, attn_mask)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 106\u001b[0m     new_x, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(new_x)\n\u001b[0;32m    111\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecomp1(x)\n",
      "File \u001b[1;32mc:\\Users\\osahl\\miniconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\osahl\\miniconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\osahl\\Desktop\\UNI\\master thesis\\FEDFormer\\layers\\AutoCorrelation.py:168\u001b[0m, in \u001b[0;36mAutoCorrelationLayer.forward\u001b[1;34m(self, queries, keys, values, attn_mask)\u001b[0m\n\u001b[0;32m    165\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_projection(keys)\u001b[38;5;241m.\u001b[39mview(B, S, H, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    166\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_projection(values)\u001b[38;5;241m.\u001b[39mview(B, S, H, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 168\u001b[0m out, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_correlation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(B, L, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_projection(out), attn\n",
      "File \u001b[1;32mc:\\Users\\osahl\\miniconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\osahl\\miniconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\osahl\\Desktop\\UNI\\master thesis\\FEDFormer\\layers\\FourierCorrelation.py:58\u001b[0m, in \u001b[0;36mFourierBlock.forward\u001b[1;34m(self, q, k, v, mask)\u001b[0m\n\u001b[0;32m     56\u001b[0m out_ft \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(B, H, E, L \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcfloat)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m wi, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m---> 58\u001b[0m     out_ft[:, :, :, wi] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompl_mul1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_ft\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Return to time domain\u001b[39;00m\n\u001b[0;32m     60\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mirfft(out_ft, n\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\osahl\\Desktop\\UNI\\master thesis\\FEDFormer\\layers\\FourierCorrelation.py:47\u001b[0m, in \u001b[0;36mFourierBlock.compl_mul1d\u001b[1;34m(self, input, weights)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompl_mul1d\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, weights):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbhi,hio->bho\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\osahl\\miniconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\functional.py:407\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    409\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "\n",
    "if args.is_training:\n",
    "    print('hej')\n",
    "    for m in ['FEDformer']:\n",
    "        args.model = m\n",
    "        for b in [8]:\n",
    "            args.batch_size = b\n",
    "            for i in range(14, 238):\n",
    "                    # Set augments by using data name\n",
    "                    args.idx = i\n",
    "                    j = str(i)\n",
    "                    while(len(j)<3):\n",
    "                        j = '0' + j\n",
    "                    for ii in range(args.itr):\n",
    "                        # setting record of experiments\n",
    "                        setting = '{}_{}_{}_{}_batchsize{}_modes{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "                            j,\n",
    "                            args.task_id,\n",
    "                            args.model,\n",
    "                            args.batch_size,\n",
    "                            args.mode_select,\n",
    "                            args.modes,\n",
    "                            args.data,\n",
    "                            args.features,\n",
    "                            args.seq_len,\n",
    "                            args.label_len,\n",
    "                            args.pred_len,\n",
    "                            args.d_model,\n",
    "                            args.n_heads,\n",
    "                            args.e_layers,\n",
    "                            args.d_layers,\n",
    "                            args.d_ff,\n",
    "                            args.factor,\n",
    "                            args.embed,\n",
    "                            args.distil,\n",
    "                            args.des,\n",
    "                            ii)\n",
    "\n",
    "                        exp = Exp(args)  # set experiments\n",
    "                        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "                        exp.train(setting)\n",
    "\n",
    "                        if args.do_predict:\n",
    "                            print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "                            exp.predict(setting, True)\n",
    "                            for i in os.listdir(\"C:/Users/osahl/Desktop/UNI/master thesis/FEDFormer/informer_checkpoints/\"):\n",
    "                                shutil.rmtree('C:/Users/osahl/Desktop/UNI/master thesis/FEDFormer/informer_checkpoints/' + i)\n",
    "\n",
    "                        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Austria</th>\n",
       "      <th>Burundi</th>\n",
       "      <th>Belgium</th>\n",
       "      <th>Burkina Faso</th>\n",
       "      <th>Bahamas</th>\n",
       "      <th>Bolivia</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Barbados</th>\n",
       "      <th>Botswana</th>\n",
       "      <th>...</th>\n",
       "      <th>Uruguay</th>\n",
       "      <th>United States</th>\n",
       "      <th>Samoa</th>\n",
       "      <th>South Africa</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Africa</th>\n",
       "      <th>North America</th>\n",
       "      <th>South America</th>\n",
       "      <th>Asia &amp; Oceania</th>\n",
       "      <th>Global Factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980-02-01</td>\n",
       "      <td>0.872943</td>\n",
       "      <td>-1.005034</td>\n",
       "      <td>0.753035</td>\n",
       "      <td>-0.435968</td>\n",
       "      <td>0.824407</td>\n",
       "      <td>0.374450</td>\n",
       "      <td>4.513792</td>\n",
       "      <td>-0.203252</td>\n",
       "      <td>3.546852</td>\n",
       "      <td>...</td>\n",
       "      <td>4.302900</td>\n",
       "      <td>1.403974</td>\n",
       "      <td>3.975538</td>\n",
       "      <td>0.985237</td>\n",
       "      <td>0.992071</td>\n",
       "      <td>1.302618</td>\n",
       "      <td>2.107086</td>\n",
       "      <td>1.835498</td>\n",
       "      <td>3.843790</td>\n",
       "      <td>2.017661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980-03-01</td>\n",
       "      <td>0.337436</td>\n",
       "      <td>0.201816</td>\n",
       "      <td>0.182826</td>\n",
       "      <td>2.454607</td>\n",
       "      <td>0.926185</td>\n",
       "      <td>1.646262</td>\n",
       "      <td>5.862831</td>\n",
       "      <td>1.714616</td>\n",
       "      <td>0.526594</td>\n",
       "      <td>...</td>\n",
       "      <td>1.950554</td>\n",
       "      <td>1.509454</td>\n",
       "      <td>1.875400</td>\n",
       "      <td>0.975625</td>\n",
       "      <td>0.764569</td>\n",
       "      <td>1.295151</td>\n",
       "      <td>1.684490</td>\n",
       "      <td>1.933483</td>\n",
       "      <td>0.927116</td>\n",
       "      <td>1.091811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980-04-01</td>\n",
       "      <td>0.432173</td>\n",
       "      <td>1.202419</td>\n",
       "      <td>0.127767</td>\n",
       "      <td>-0.534332</td>\n",
       "      <td>1.025097</td>\n",
       "      <td>0.861332</td>\n",
       "      <td>5.151259</td>\n",
       "      <td>0.895974</td>\n",
       "      <td>0.104987</td>\n",
       "      <td>...</td>\n",
       "      <td>2.533744</td>\n",
       "      <td>1.117331</td>\n",
       "      <td>5.112935</td>\n",
       "      <td>0.966198</td>\n",
       "      <td>1.321007</td>\n",
       "      <td>0.630528</td>\n",
       "      <td>1.128067</td>\n",
       "      <td>2.299137</td>\n",
       "      <td>1.167593</td>\n",
       "      <td>1.133593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980-05-01</td>\n",
       "      <td>0.478009</td>\n",
       "      <td>0.298359</td>\n",
       "      <td>0.291456</td>\n",
       "      <td>-1.348456</td>\n",
       "      <td>0.642057</td>\n",
       "      <td>2.978359</td>\n",
       "      <td>5.546105</td>\n",
       "      <td>0.395648</td>\n",
       "      <td>-0.210084</td>\n",
       "      <td>...</td>\n",
       "      <td>3.410773</td>\n",
       "      <td>0.982813</td>\n",
       "      <td>0.517868</td>\n",
       "      <td>0.956931</td>\n",
       "      <td>0.576158</td>\n",
       "      <td>1.150624</td>\n",
       "      <td>1.085661</td>\n",
       "      <td>2.255788</td>\n",
       "      <td>0.751945</td>\n",
       "      <td>0.828821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980-06-01</td>\n",
       "      <td>1.090843</td>\n",
       "      <td>-0.198807</td>\n",
       "      <td>0.236171</td>\n",
       "      <td>0.838183</td>\n",
       "      <td>0.849713</td>\n",
       "      <td>5.378239</td>\n",
       "      <td>5.169338</td>\n",
       "      <td>2.051855</td>\n",
       "      <td>2.594852</td>\n",
       "      <td>...</td>\n",
       "      <td>3.567083</td>\n",
       "      <td>1.094237</td>\n",
       "      <td>2.348245</td>\n",
       "      <td>1.886842</td>\n",
       "      <td>0.530069</td>\n",
       "      <td>1.757311</td>\n",
       "      <td>1.039153</td>\n",
       "      <td>1.731945</td>\n",
       "      <td>0.823665</td>\n",
       "      <td>0.916747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>0.093949</td>\n",
       "      <td>0.818558</td>\n",
       "      <td>-0.018337</td>\n",
       "      <td>-1.503201</td>\n",
       "      <td>-0.386421</td>\n",
       "      <td>0.500315</td>\n",
       "      <td>0.109999</td>\n",
       "      <td>1.921638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880221</td>\n",
       "      <td>-0.004989</td>\n",
       "      <td>-0.081060</td>\n",
       "      <td>0.219975</td>\n",
       "      <td>0.099504</td>\n",
       "      <td>1.664324</td>\n",
       "      <td>0.238937</td>\n",
       "      <td>0.279448</td>\n",
       "      <td>0.519181</td>\n",
       "      <td>0.810713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>0.468424</td>\n",
       "      <td>0.901719</td>\n",
       "      <td>-0.459999</td>\n",
       "      <td>-0.206744</td>\n",
       "      <td>0.303742</td>\n",
       "      <td>-0.081468</td>\n",
       "      <td>-0.039978</td>\n",
       "      <td>0.972621</td>\n",
       "      <td>0.098280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514474</td>\n",
       "      <td>0.078310</td>\n",
       "      <td>0.082406</td>\n",
       "      <td>0.329139</td>\n",
       "      <td>0.129016</td>\n",
       "      <td>1.293934</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>0.220231</td>\n",
       "      <td>0.216114</td>\n",
       "      <td>0.607386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>0.186757</td>\n",
       "      <td>0.537154</td>\n",
       "      <td>0.358990</td>\n",
       "      <td>0.540568</td>\n",
       "      <td>-0.451343</td>\n",
       "      <td>0.448842</td>\n",
       "      <td>0.100039</td>\n",
       "      <td>0.406711</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749476</td>\n",
       "      <td>0.228301</td>\n",
       "      <td>-0.185927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177487</td>\n",
       "      <td>0.958882</td>\n",
       "      <td>0.322159</td>\n",
       "      <td>0.476384</td>\n",
       "      <td>0.510649</td>\n",
       "      <td>0.592476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>0.186409</td>\n",
       "      <td>0.711747</td>\n",
       "      <td>0.064324</td>\n",
       "      <td>-0.747741</td>\n",
       "      <td>0.009231</td>\n",
       "      <td>1.108687</td>\n",
       "      <td>0.508696</td>\n",
       "      <td>0.506074</td>\n",
       "      <td>0.098087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419433</td>\n",
       "      <td>-0.053617</td>\n",
       "      <td>-0.119933</td>\n",
       "      <td>0.109446</td>\n",
       "      <td>-0.024420</td>\n",
       "      <td>1.475937</td>\n",
       "      <td>0.425120</td>\n",
       "      <td>0.265014</td>\n",
       "      <td>0.250176</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>0.649615</td>\n",
       "      <td>1.321023</td>\n",
       "      <td>0.128431</td>\n",
       "      <td>0.522040</td>\n",
       "      <td>-0.769128</td>\n",
       "      <td>-1.551573</td>\n",
       "      <td>1.143537</td>\n",
       "      <td>0.754341</td>\n",
       "      <td>0.195886</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029549</td>\n",
       "      <td>-0.090993</td>\n",
       "      <td>-0.812882</td>\n",
       "      <td>0.218614</td>\n",
       "      <td>0.145798</td>\n",
       "      <td>1.010496</td>\n",
       "      <td>0.296846</td>\n",
       "      <td>0.058492</td>\n",
       "      <td>0.179311</td>\n",
       "      <td>0.503098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date   Austria   Burundi   Belgium  Burkina Faso   Bahamas  \\\n",
       "0    1980-02-01  0.872943 -1.005034  0.753035     -0.435968  0.824407   \n",
       "1    1980-03-01  0.337436  0.201816  0.182826      2.454607  0.926185   \n",
       "2    1980-04-01  0.432173  1.202419  0.127767     -0.534332  1.025097   \n",
       "3    1980-05-01  0.478009  0.298359  0.291456     -1.348456  0.642057   \n",
       "4    1980-06-01  1.090843 -0.198807  0.236171      0.838183  0.849713   \n",
       "..          ...       ...       ...       ...           ...       ...   \n",
       "474  2019-08-01  0.093949  0.818558 -0.018337     -1.503201 -0.386421   \n",
       "475  2019-09-01  0.468424  0.901719 -0.459999     -0.206744  0.303742   \n",
       "476  2019-10-01  0.186757  0.537154  0.358990      0.540568 -0.451343   \n",
       "477  2019-11-01  0.186409  0.711747  0.064324     -0.747741  0.009231   \n",
       "478  2019-12-01  0.649615  1.321023  0.128431      0.522040 -0.769128   \n",
       "\n",
       "      Bolivia    Brazil  Barbados  Botswana  ...   Uruguay  United States  \\\n",
       "0    0.374450  4.513792 -0.203252  3.546852  ...  4.302900       1.403974   \n",
       "1    1.646262  5.862831  1.714616  0.526594  ...  1.950554       1.509454   \n",
       "2    0.861332  5.151259  0.895974  0.104987  ...  2.533744       1.117331   \n",
       "3    2.978359  5.546105  0.395648 -0.210084  ...  3.410773       0.982813   \n",
       "4    5.378239  5.169338  2.051855  2.594852  ...  3.567083       1.094237   \n",
       "..        ...       ...       ...       ...  ...       ...            ...   \n",
       "474  0.500315  0.109999  1.921638  0.000000  ...  0.880221      -0.004989   \n",
       "475 -0.081468 -0.039978  0.972621  0.098280  ...  0.514474       0.078310   \n",
       "476  0.448842  0.100039  0.406711  0.098184  ...  0.749476       0.228301   \n",
       "477  1.108687  0.508696  0.506074  0.098087  ...  0.419433      -0.053617   \n",
       "478 -1.551573  1.143537  0.754341  0.195886  ... -0.029549      -0.090993   \n",
       "\n",
       "        Samoa  South Africa    Europe    Africa  North America  South America  \\\n",
       "0    3.975538      0.985237  0.992071  1.302618       2.107086       1.835498   \n",
       "1    1.875400      0.975625  0.764569  1.295151       1.684490       1.933483   \n",
       "2    5.112935      0.966198  1.321007  0.630528       1.128067       2.299137   \n",
       "3    0.517868      0.956931  0.576158  1.150624       1.085661       2.255788   \n",
       "4    2.348245      1.886842  0.530069  1.757311       1.039153       1.731945   \n",
       "..        ...           ...       ...       ...            ...            ...   \n",
       "474 -0.081060      0.219975  0.099504  1.664324       0.238937       0.279448   \n",
       "475  0.082406      0.329139  0.129016  1.293934       0.186667       0.220231   \n",
       "476 -0.185927      0.000000  0.177487  0.958882       0.322159       0.476384   \n",
       "477 -0.119933      0.109446 -0.024420  1.475937       0.425120       0.265014   \n",
       "478 -0.812882      0.218614  0.145798  1.010496       0.296846       0.058492   \n",
       "\n",
       "     Asia & Oceania  Global Factor  \n",
       "0          3.843790       2.017661  \n",
       "1          0.927116       1.091811  \n",
       "2          1.167593       1.133593  \n",
       "3          0.751945       0.828821  \n",
       "4          0.823665       0.916747  \n",
       "..              ...            ...  \n",
       "474        0.519181       0.810713  \n",
       "475        0.216114       0.607386  \n",
       "476        0.510649       0.592476  \n",
       "477        0.250176       0.707107  \n",
       "478        0.179311       0.503098  \n",
       "\n",
       "[479 rows x 98 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(args.root_path + args.data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
