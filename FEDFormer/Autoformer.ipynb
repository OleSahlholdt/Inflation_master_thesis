{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import dotdict\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "args.model = 'FEDformer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "args.task_id = 'informer'\n",
    "args.is_training = 1\n",
    "args.version = 'Fourier'\n",
    "args.task = 'test'\n",
    "args.data = 'custom' # data\n",
    "args.root_path = './dataset/' # root path of data file\n",
    "args.data_path = 'Informer-data.csv' # data file\n",
    "args.features = 'M' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.target = 'Austria' # target feature in S or MS task\n",
    "args.continent = 'Europe'\n",
    "args.freq = 'm' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.detail_freq = args.freq\n",
    "args.freq = args.freq[-1:]\n",
    "args.checkpoints = './Autoformer_checkpoints' # location of model checkpoints\n",
    "\n",
    "args.mode_select = 'random'\n",
    "args.modes = 32\n",
    "args.seq_len = 32 # input sequence length of Informer encoder\n",
    "args.label_len = 16# start token length of Informer decoder\n",
    "args.pred_len = 12 # prediction sequence length\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "args.enc_in = 97 # encoder input size\n",
    "args.dec_in = 97 # decoder input size\n",
    "args.c_out  = 97 # output size\n",
    "args.factor = 2 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 8 # num of heads\n",
    "args.e_layers = 2 # num of encoder layers\n",
    "args.d_layers = 2 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.1 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.do_predict = True\n",
    "args.moving_avg = 12\n",
    "args.cross_activation = 'tanh'\n",
    "args.base = 'legendre'\n",
    "\n",
    "args.batch_size = 16\n",
    "args.learning_rate = 0.0001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False # whether to use automatic mixed precision training\n",
    "\n",
    "args.num_workers = 0\n",
    "args.itr = 1\n",
    "args.train_epochs = 8\n",
    "args.patience = 3\n",
    "args.des = 'exp'\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() else False\n",
    "args.gpu = 0\n",
    "\n",
    "args.use_multi_gpu = False\n",
    "args.devices = '0,1,2,3'\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae5d5a3eb754f23844a3a28583de2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "Autocorrelation used !\n",
      "Autocorrelation used !\n",
      "Autocorrelation used !\n",
      "Autocorrelation used !\n",
      "Autocorrelation used !\n",
      "Autocorrelation used !\n",
      ">>>>>>>start training : 000_Autoformer_h1_seqlen6_labellen6_heads8_encoderlayers2_dm512_ma5_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 199\n",
      "val 13\n",
      "test 0\n",
      "Epoch: 1 cost time: 1.2413668632507324\n",
      "Epoch: 1, Steps: 13 | Train Loss: 1.3222940 Vali Loss: 1.0448856 Test Loss: nan\n",
      "Validation loss decreased (inf --> 1.044886).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "Epoch: 2 cost time: 1.013730764389038\n",
      "Epoch: 2, Steps: 13 | Train Loss: 1.0967489 Vali Loss: 0.9805055 Test Loss: nan\n",
      "Validation loss decreased (1.044886 --> 0.980506).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "Epoch: 3 cost time: 0.9791073799133301\n",
      "Epoch: 3, Steps: 13 | Train Loss: 0.9993817 Vali Loss: 0.9574687 Test Loss: nan\n",
      "Validation loss decreased (0.980506 --> 0.957469).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "Epoch: 4 cost time: 0.8316688537597656\n",
      "Epoch: 4, Steps: 13 | Train Loss: 0.9521176 Vali Loss: 0.9510688 Test Loss: nan\n",
      "Validation loss decreased (0.957469 --> 0.951069).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "Epoch: 5 cost time: 0.8525872230529785\n",
      "Epoch: 5, Steps: 13 | Train Loss: 0.9440840 Vali Loss: 0.9484345 Test Loss: nan\n",
      "Validation loss decreased (0.951069 --> 0.948435).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "Epoch: 6 cost time: 0.8281025886535645\n",
      "Epoch: 6, Steps: 13 | Train Loss: 0.9312147 Vali Loss: 0.9477674 Test Loss: nan\n",
      "Validation loss decreased (0.948435 --> 0.947767).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "Epoch: 7 cost time: 0.8932476043701172\n",
      "Epoch: 7, Steps: 13 | Train Loss: 0.9238925 Vali Loss: 0.9468361 Test Loss: nan\n",
      "Validation loss decreased (0.947767 --> 0.946836).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "Epoch: 8 cost time: 1.0130176544189453\n",
      "Epoch: 8, Steps: 13 | Train Loss: 0.9251465 Vali Loss: 0.9473535 Test Loss: nan\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "val 13\n",
      "Validation Loss 0.9468361139297485, loss1 0.9468361139297485\n",
      "New best model with loss: 0.9468361139297485\n",
      "0.9468361\n",
      "{'model': 'Autoformer', 'task_id': 'Autoformer_h1', 'is_training': 1, 'version': 'Fourier', 'task': 'test', 'data': 'custom', 'root_path': './dataset/', 'data_path': 'Informer-data.csv', 'features': 'M', 'target': 'Austria', 'continent': 'Europe', 'freq': 'm', 'detail_freq': 'm', 'checkpoints': './Autoformer_checkpoints', 'mode_select': 'random', 'modes': 32, 'seq_len': 6, 'label_len': 6, 'pred_len': 12, 'enc_in': 97, 'dec_in': 97, 'c_out': 97, 'factor': 2, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.1, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'do_predict': True, 'moving_avg': 5, 'cross_activation': 'tanh', 'base': 'legendre', 'batch_size': 16, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 8, 'patience': 3, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'idx': 0}\n",
      "000_Autoformer_h1_seqlen6_labellen6_heads8_encoderlayers2_dm512_ma5_0\n",
      ">>>>>>>start prediction_training : 000_Autoformer_h1_seqlen6_labellen6_heads8_encoderlayers2_dm512_ma5_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>>predicting : 000_Autoformer_h1_seqlen6_labellen6_heads8_encoderlayers2_dm512_ma5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "pred 1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:/Users/osahl/Desktop/UNI/master thesis/FEDFormer/Autoformer_checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>predicting : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(best_setting))\n\u001b[0;32m     61\u001b[0m exp\u001b[38;5;241m.\u001b[39mpredict(best_setting, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ckpt \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/osahl/Desktop/UNI/master thesis/FEDFormer/Autoformer_checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     63\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mrmtree(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/osahl/Desktop/UNI/master thesis/FEDFormer/Autoformer_checkpoints/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:/Users/osahl/Desktop/UNI/master thesis/FEDFormer/Autoformer_checkpoints'"
     ]
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "seq_lengths = [6]#, 12, 24, 48]\n",
    "\n",
    "args.task_id = \"Autoformer_h1\"\n",
    "\n",
    "if args.is_training:\n",
    "    for m in ['Autoformer']:\n",
    "        args.model = m\n",
    "        for i in tqdm(range(0, 238)):\n",
    "            best_loss = float('inf')\n",
    "            for kernel_size in [5]:#, 9, 13, 25]:\n",
    "                args.moving_avg = kernel_size\n",
    "                for idx, seq_length in enumerate(seq_lengths):\n",
    "                    args.seq_len = seq_length\n",
    "                    for label_length in seq_lengths[:idx + 1]:\n",
    "                        args.label_len = label_length\n",
    "                        # Set augments by using data name\n",
    "                        args.idx = i\n",
    "                        j = str(i)\n",
    "                        while(len(j)<3):\n",
    "                            j = '0' + j\n",
    "                        for ii in range(args.itr):\n",
    "                            # setting record of experiments\n",
    "                            setting = '{}_{}_seqlen{}_labellen{}_heads{}_encoderlayers{}_dm{}_ma{}_{}'.format(\n",
    "                                j,\n",
    "                                args.task_id,\n",
    "                                args.seq_len,\n",
    "                                args.label_len,\n",
    "                                args.n_heads,\n",
    "                                args.e_layers,\n",
    "                                args.d_model,\n",
    "                                args.moving_avg,\n",
    "                                ii)\n",
    "\n",
    "                            exp = Exp(args)  # set experiments\n",
    "                            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "                            try:\n",
    "                                exp.train(setting)\n",
    "                            except Exception as e:\n",
    "                                print(f\"===========================GOT ERROR: {e}\")\n",
    "                                continue\n",
    "                            vali_data, vali_loader = exp._get_data(flag='val')\n",
    "                            loss = exp.vali(vali_data, vali_loader, exp._select_criterion())\n",
    "                            loss1 = exp.vali(vali_data, vali_loader, exp._select_criterion())\n",
    "                            print(f\"Validation Loss {loss}, loss1 {loss1}\")\n",
    "                            torch.cuda.empty_cache()\n",
    "                            if loss < best_loss:\n",
    "                                print(f\"New best model with loss: {loss}\")\n",
    "                                best_loss = loss\n",
    "                                best_args = args\n",
    "                                best_setting = setting\n",
    "                            print(best_loss)\n",
    "                            print(best_args)\n",
    "                            print(best_setting)\n",
    "\n",
    "            if args.do_predict:\n",
    "                #exp = Exp(best_args)  # set experiments\n",
    "                print('>>>>>>>start prediction_training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(best_setting))\n",
    "                #exp.train(best_setting)\n",
    "                print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(best_setting))\n",
    "                exp.predict(best_setting, True)\n",
    "                for ckpt in os.listdir(r\"C:\\Users\\osahl\\Desktop\\UNI\\master thesis\\Transformer-Master-Thesis\\FEDFormer\\Autoformer_checkpoints\"):\n",
    "                    shutil.rmtree(rf'C:/Users/osahl/Desktop/UNI/master thesis/FEDFormer/Autoformer_checkpoints/{ckpt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Main\n",
    "seq_lengths = [6, 12, 24, 48]\n",
    "\n",
    "args.task_id = \"Autoformer_h6\"\n",
    "args.pred_len = 6\n",
    "\n",
    "if args.is_training:\n",
    "    for m in ['Autoformer']:\n",
    "        args.model = m\n",
    "        for i in tqdm(range(0, 238)):\n",
    "            best_loss = float('inf')\n",
    "            for kernel_size in [5, 9, 13, 25]:\n",
    "                args.moving_avg = kernel_size\n",
    "                for idx, seq_length in enumerate(seq_lengths):\n",
    "                    args.seq_len = seq_length\n",
    "                    for label_length in seq_lengths[:idx + 1]:\n",
    "                        args.label_len = label_length\n",
    "                        # Set augments by using data name\n",
    "                        args.idx = i\n",
    "                        j = str(i)\n",
    "                        while(len(j)<3):\n",
    "                            j = '0' + j\n",
    "                        for ii in range(args.itr):\n",
    "                            # setting record of experiments\n",
    "                            setting = '{}_{}_seqlen{}_labellen{}_heads{}_encoderlayers{}_model{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}__ma{}_{}'.format(\n",
    "                                j,\n",
    "                                args.task_id,\n",
    "                                args.seq_len,\n",
    "                                args.label_len,\n",
    "                                args.n_heads,\n",
    "                                args.e_layers,\n",
    "                                args.model,\n",
    "                                args.batch_size,\n",
    "                                args.mode_select,\n",
    "                                args.modes,\n",
    "                                args.data,\n",
    "                                args.features,\n",
    "                                args.pred_len,\n",
    "                                args.d_model,\n",
    "                                args.d_layers,\n",
    "                                args.d_ff,\n",
    "                                args.factor,\n",
    "                                args.embed,\n",
    "                                args.distil,\n",
    "                                args.des,\n",
    "                                args.moving_avg,\n",
    "                                ii)\n",
    "\n",
    "                            exp = Exp(args)  # set experiments\n",
    "                            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "                            try:\n",
    "                                exp.train(setting)\n",
    "                            except Exception as e:\n",
    "                                print(f\"===========================GOT ERROR: {e}\")\n",
    "                                continue\n",
    "                            vali_data, vali_loader = exp._get_data(flag='val')\n",
    "                            loss = exp.vali(vali_data, vali_loader, exp._select_criterion())\n",
    "                            torch.cuda.empty_cache()\n",
    "                            if loss < best_loss:\n",
    "                                print(f\"New best model with loss: {loss}\")\n",
    "                                best_loss = loss\n",
    "                                best_args = args\n",
    "                                best_setting = setting\n",
    "                            print(best_loss)\n",
    "                            print(best_args)\n",
    "                            print(best_setting)\n",
    "\n",
    "            if args.do_predict:\n",
    "                #exp = Exp(best_args)  # set experiments\n",
    "                print('>>>>>>>start prediction_training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(best_setting))\n",
    "                #exp.train(best_setting)\n",
    "                print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(best_setting))\n",
    "                exp.predict(best_setting, True)\n",
    "                for ckpt in os.listdir(\"C:/Users/osahl/Desktop/UNI/master thesis/FEDFormer/Autoformer_checkpoints\"):\n",
    "                    shutil.rmtree(f'C:/Users/osahl/Desktop/UNI/master thesis/FEDFormer/Autoformer_checkpoints/{ckpt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Main\n",
    "seq_lengths = [6, 12, 24, 48]\n",
    "\n",
    "args.task_id = \"Autoformer_h12\"\n",
    "args.pred_len = 12\n",
    "\n",
    "if args.is_training:\n",
    "    for m in ['Autoformer']:\n",
    "        args.model = m\n",
    "        for i in tqdm(range(0, 238)):\n",
    "            best_loss = float('inf')\n",
    "            for kernel_size in [5, 9, 13, 25]:\n",
    "                args.moving_avg = kernel_size\n",
    "                for idx, seq_length in enumerate(seq_lengths):\n",
    "                    args.seq_len = seq_length\n",
    "                    for label_length in seq_lengths[:idx + 1]:\n",
    "                        args.label_len = label_length\n",
    "                        # Set augments by using data name\n",
    "                        args.idx = i\n",
    "                        j = str(i)\n",
    "                        while(len(j)<3):\n",
    "                            j = '0' + j\n",
    "                        for ii in range(args.itr):\n",
    "                            # setting record of experiments\n",
    "                            setting = '{}_{}_seqlen{}_labellen{}_heads{}_encoderlayers{}_model{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}__ma{}_{}'.format(\n",
    "                                j,\n",
    "                                args.task_id,\n",
    "                                args.seq_len,\n",
    "                                args.label_len,\n",
    "                                args.n_heads,\n",
    "                                args.e_layers,\n",
    "                                args.model,\n",
    "                                args.batch_size,\n",
    "                                args.mode_select,\n",
    "                                args.modes,\n",
    "                                args.data,\n",
    "                                args.features,\n",
    "                                args.pred_len,\n",
    "                                args.d_model,\n",
    "                                args.d_layers,\n",
    "                                args.d_ff,\n",
    "                                args.factor,\n",
    "                                args.embed,\n",
    "                                args.distil,\n",
    "                                args.des,\n",
    "                                args.moving_avg,\n",
    "                                ii)\n",
    "\n",
    "                            exp = Exp(args)  # set experiments\n",
    "                            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "                            try:\n",
    "                                exp.train(setting)\n",
    "                            except Exception as e:\n",
    "                                print(f\"===========================GOT ERROR: {e}\")\n",
    "                                continue\n",
    "                            vali_data, vali_loader = exp._get_data(flag='val')\n",
    "                            loss = exp.vali(vali_data, vali_loader, exp._select_criterion())\n",
    "                            torch.cuda.empty_cache()\n",
    "                            if loss < best_loss:\n",
    "                                print(f\"New best model with loss: {loss}\")\n",
    "                                best_loss = loss\n",
    "                                best_args = args\n",
    "                                best_setting = setting\n",
    "                            print(best_loss)\n",
    "                            print(best_args)\n",
    "                            print(best_setting)\n",
    "\n",
    "            if args.do_predict:\n",
    "                #exp = Exp(best_args)  # set experiments\n",
    "                print('>>>>>>>start prediction_training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(best_setting))\n",
    "                #exp.train(best_setting)\n",
    "                print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(best_setting))\n",
    "                exp.predict(best_setting, True)\n",
    "                for ckpt in os.listdir(\"C:/Users/osahl/Desktop/UNI/master thesis/FEDFormer/Autoformer_checkpoints\"):\n",
    "                    shutil.rmtree(f'C:/Users/osahl/Desktop/UNI/master thesis/FEDFormer/Autoformer_checkpoints/{ckpt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
